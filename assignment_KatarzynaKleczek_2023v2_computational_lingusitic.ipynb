{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbfead40",
   "metadata": {},
   "source": [
    "# Spelling correction\n",
    "\n",
    "You receive the following files, available via SurfDrive:\n",
    "- sentences_with_typos.txt: a file with two tab-separated columns, the first containing a numerical ID and the second containing a sentence in English\n",
    "- SUBTLEXus.txt: a file with several tab-separated columns, containing data from the SUBTLEXus lexicon, a list of words derived from a part of the SUBTLEXus corpus containing movie subtitles with their frequency counts (and other pieces of information)\n",
    "- LM_trainingCorpus.json: a file containing a pre-processed corpus in the form of a list of lists of strings, to be used to train a word-level statistical language model\n",
    "\n",
    "You should carry out the following tasks:\n",
    "\n",
    "- task1: comment the code to estimate the LM by adding doc-strings. 4 points available, you lose 1 point for every missing docstring. There are 6 docstrings to write, so writing 2 give you the same points as writing none: the rationale behind the grading approach is that writing only 2 is not a sufficient demonstration you understand the code.\n",
    "\n",
    "\n",
    "- task2a: After reading in the files as pandas dataframes (mind how you import default strings, some of them may turn into NAs!), find which token contains a typo in the given sentences, where having a typo means that the word does not feature in SUBTLEXus - remember to tokenise the input sentences! There is one and only one word containing a typo in each sentence, as defined in this way: the typo can result from the insertion/deletion/substitution of one or two characters. You should submit a .json file containing a simple dictionary mapping the sentence ID (as an integer) to the mistyped word (as a string). 5 points available in total: for every incorrect word retrieved, you lose 0.5 point. If you retrieve 10 wrong mistyped words, you get no points even if the total number of mistyped words is higher than 10. If you submit a wrongly formatted file, you will get no points.\n",
    "\n",
    "\n",
    "- task2b: read the target sentences manually. Some of them contain another mistyped word than the one you found in task2a, but were not detected because the typo resulted in a word which appears in SUBTLEXus. Discuss how you could automatically spot those mistyped words too using CL methods and resources: what information would you need? How would you use it? Reply in no more than 150 words. 3 points available, awarded based on how sensible the answer is.\n",
    "\n",
    "\n",
    "- task3: find the words from SUBTLEXus with the smallest edit distance from each mistyped target (do not lowercase anything). You should return the 3 words at the smallest edit distance, sorted by edit distance. However, if there are more words at the same edit distance than the third closest, you should include all the words at the same edit distance. Therefore, supposing that the string 'abcdef' has two neighbors at edit distance 1, and four neighbors at edit distance 2, the third closest neighbor would be at edit distance 2, but there would be other three words at the same distance and you should thus return six neighbors for the target string. 5 points available: you loose 0.5 points for every wrong list of nearest neighbors you retrieve for a mistyped word (wrong means any of wrong words, wrong order, wrong edit distances, wrong data type). Submit a .json file containing a dictionary mapping sentence IDs (as integers) to lists of tuples, where each tuple contains first the word (as a string) and then the edit distance (as an integer), with tuples sorted by edit distance in ascending order (smallest edit distance first). The dictionary should have the following form (if you submit a wrongly formatted file, you will get no points):\n",
    "    {id1: [(w1, 2), (w2, 2), (w3, 2)];\n",
    "     id2: [(w6, 1), (w7, 1), (w8, 2), (w9, 2), (w10, 2)];\n",
    "     ...}\n",
    "     \n",
    "     \n",
    "- task4: use the list of candidate replacements you found in task3 to find the best one according to candidate frequency (derived from SUBTLEXus) - if two or more candidates have the exact same frequency in SUBTLEX, choose the one with the best edit distance. If two or more candidates at the same frequency also have the same edit distance, pick the one which comes first in alphabetical order. You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its SUBTLEXus frequency value (as an integer). 5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the frequency doesn't match, you loose half a point).\n",
    "\n",
    "\n",
    "- task5: use the list of candidate replacements you found in task3 to find the best one according to its perplexity under a statistical language model of order 3 implemented using a Markov Chain with add-k smoothing (k=0.01) and estimated using the given corpus. If two or more candidates have the exact same perplexity in the input sentence, choose the one with the best edit distance. If two or more candidates at the same perplexity also have the same edit distance, follow alphabetical order. You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its perplexity under the specified language model (as a float). Not all candidate replacements might appear in the LM training corpus: don't map such candidate replacements to the UNK string though, or the perplexity estimate would not reflect the specific candidate; you can directly exclude candidate replacements which don't appear in the training corpus from the perplexity computation. 5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the perplexity doesn't match, you loose half a point).\n",
    "\n",
    "\n",
    "- task6: Compare the candidate replacements found when considering frequency and when considering perplexity. Which are better? Do they match what you consider to be the right replacement? What extra resources/information would you use to pick better candidate replacements? 3 points available, awarded based on how sensible the answer is.\n",
    "\n",
    "\n",
    "Name files as follows:\n",
    "task[n]_NameSurname_solution.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60a274bd",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "In the cell below you find code to run a statistical language model, add the docstrings (you already find a blueprint) to complete the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c603a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class LM(object):\n",
    "    \n",
    "    def __init__(self, corpus, ngram_size=2, bos='+', eos='#', k=1):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param corpus: corpus is the dataset that will be used to derive information from\n",
    "        :param ngram_size: ngram size is the size of n_gram the brick to build the simplest language models, such as naive Bayes. 2 for bigrams, 3 for trigrams, and so on.\n",
    "        :param k: smootihng parameter \n",
    "        :param bos: is the beginning of sentence symbol\n",
    "        :param eos: end of sentence symbol \n",
    "        \n",
    "        This class creates an LM object with attributes k,ngram_size,bos,eos,corpus,vocab (a set with only distinct tokens),vocab_size (the lenght of the vocabulary set)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.k = k\n",
    "        self.ngram_size = ngram_size\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.corpus = corpus\n",
    "        self.vocab = self.get_vocab()\n",
    "        self.vocab.add(self.eos)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        :return: tokens\n",
    "        \n",
    "        This method derives distinct tokens from the corpus \n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = set()\n",
    "        for sentence in self.corpus:\n",
    "            for element in sentence:\n",
    "                vocab.add(element)\n",
    "        \n",
    "        return vocab\n",
    "                    \n",
    "    def update_counts(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        :return: nothing, it is only updating a variable\n",
    "        This method creates a way to update a dictionary with the count of the ouccrances of a word given specific history (n-gram) \n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.ngram_size - 1\n",
    "        \n",
    "        self.counts = defaultdict(dict)\n",
    "        \n",
    "        for sentence in self.corpus:\n",
    "            s = [self.bos]*r + sentence + [self.eos]\n",
    "            \n",
    "            for idx in range(self.ngram_size-1, len(s)):\n",
    "                ngram = self.get_ngram(s, idx)\n",
    "                \n",
    "                try:\n",
    "                    self.counts[ngram[0]][ngram[1]] += 1\n",
    "                except KeyError:\n",
    "                    self.counts[ngram[0]][ngram[1]] = 1\n",
    "                        \n",
    "    def get_ngram(self, s, i):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param s: sentence that we will go through to get an engram \n",
    "        :param i: an iterator to know where in the sentence we want to look for the target n-gram\n",
    "        :return: the target word and n-1 proceeding words that are needed for an n-gram \n",
    "        \n",
    "        This method looks at the sentence passed and based on the iterator returns a target word with n-1 words proceeding the target one to creat and n-gram\n",
    "        \"\"\"\n",
    "        \n",
    "        ngram = s[i-(self.ngram_size-1):i+1]\n",
    "        history = tuple(ngram[:-1])\n",
    "        target = ngram[-1]\n",
    "        return (history, target)\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param history: the past occurance of given target ngram \n",
    "        :param target: ngram that we want to predict \n",
    "        :return: legal probability of the target n-gram\n",
    "        \n",
    "        This method outputs a legal probability of the target n-gram given the previously observed history\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[history].values())) + (self.vocab_size*self.k)\n",
    "            try:\n",
    "                transition_count = self.counts[history][target] + self.k\n",
    "            except KeyError:\n",
    "                transition_count = self.k\n",
    "        except KeyError:\n",
    "            transition_count = self.k\n",
    "            ngram_tot = self.vocab_size*self.k\n",
    "        \n",
    "        return transition_count/ngram_tot \n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param string: a sentence (combinations of strings) that we are interested to compute perplexity \n",
    "        ^ I think the given parameter is incorrect, there is no parametere named string\n",
    "        :return: perplexity\n",
    "        \n",
    "        This method computes the perplexity of a sentence. which is ~invers of probability. So it's average over log probabilities of the words\n",
    "        the better the language model, the lower the perplexity\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.ngram_size - 1\n",
    "        s = [self.bos]*r + sentence + [self.eos]\n",
    "        \n",
    "\n",
    "        probs = []\n",
    "        for idx in range(self.ngram_size-1, len(s)):\n",
    "            ngram = self.get_ngram(s, idx)\n",
    "            probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
    "                    \n",
    "        entropy = np.log2(probs)\n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2e97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075ec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "#pip install regex\n",
    "import regex as re\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6502f6a9",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8ea744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data_trainingCorpus = json.load(open(r'LM_trainingCorpus.json', 'r'))\n",
    "#print(data_trainingCorpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69c841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences=[]\n",
    "new_element=''\n",
    "for element in data_trainingCorpus:\n",
    "    new_element=' '.join(element)\n",
    "    training_sentences.append(new_element)\n",
    "\n",
    "#training_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263103fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencesraw = pd.read_csv(r\"sentence_with_typos.txt\", sep=\"\\t\")\n",
    "sentences = np.asarray(sentencesraw[\"sentence\"])\n",
    "sentences_tokenized = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_tokenized = nltk.wordpunct_tokenize(sentence)\n",
    "    sentences_tokenized.append(sentence_tokenized)\n",
    "\n",
    "#print(sentences_tokenized)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ead078-e304-4bec-8716-63aa750cb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_subtitlesraw = pd.read_csv(r'SUBTLEXus.txt', sep=\"\\t\",low_memory=False)\n",
    "movie_subtitle=list(np.asarray(movie_subtitlesraw[\"Word\"]))\n",
    "#movie_subtitlesraw\n",
    "#print(movie_subtitle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2d5bac9",
   "metadata": {},
   "source": [
    "## Task 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96402a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'knoq', 2: 'reserachers', 3: 'grozn', 4: 'quolg', 5: 'waies', 6: 'wintr', 7: 'munors', 8: 'surgicaly', 9: 'aquire', 10: 'acomodate', 11: 'dats', 12: 'collegue', 13: 'layed', 14: 'cate', 15: 'giambic'}\n"
     ]
    }
   ],
   "source": [
    "# find mistyped words in the input sentences\n",
    "where_are_mistakes1=dict()\n",
    "mistakes=dict()\n",
    "i=0\n",
    "while i < len(sentences_tokenized):\n",
    "    sentence = sentences_tokenized[i]\n",
    "    mistakes_i=[]\n",
    "    for element in sentence:\n",
    "        #existing word \n",
    "        if element in movie_subtitle:\n",
    "            mistakes_i.append(0)\n",
    "            pass\n",
    "        #if it is a punctiation symbol\n",
    "        elif element in string.punctuation:\n",
    "                mistakes_i.append(0)\n",
    "                pass\n",
    "        #in any other case it is a mistake, so we should add it to the dictionary\n",
    "        else:\n",
    "           mistakes_i.append(1)\n",
    "           mistakes[i+1]=element \n",
    "    possible_mistakes=np.asarray(mistakes_i)\n",
    "    misspelling_index=np.where(possible_mistakes > 0)[0][0]\n",
    "    where_are_mistakes1[i]=misspelling_index\n",
    "    i+=1\n",
    "print(mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777db8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to be written -> mistakes\n",
    "with open(\"task2a_KatarzynaKleczek_solution.json\", \"w\") as outfile:\n",
    "\tjson.dump(mistakes, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c013c0f",
   "metadata": {},
   "source": [
    "## Task2b \n",
    "\n",
    " the mistyped words are things such as fiends instead of friends, red instead of read and so on. After the \"I\" pronoun should be e.g a verb, not a noun describing color.\n",
    " To caputre that informations using CL we can use PoS tagging and then by applying a grammar CNF we could make the algorithm learn what is allowed as a sentence\n",
    " in terms of parts of speech relations, and what not (Hiddem markov models).  Mistakes such as \"our fiends had a baby\" are harder to correct. They need context, some kind of probabilty of a sentence existance (using Naive Bayes for example) analysis would be in my opinion the most succesfull attepmt to correct them. as probability of a sentence, our friends had a baby \n",
    " would be higher than fiends had a baby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a247ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['did you knoq that our fiends had a baby?'\n",
      " 'I red that reserachers managed to deviate the orbit of a comet with a satellite.'\n",
      " 'I could not help but grozn in frustration when my computer trashed right before I finished my project.'\n",
      " 'I mate a quolg from old clothes for my newborn nephew.'\n",
      " 'he waies his wand to get the attention of the waiter.'\n",
      " 'the roofs of the old house needed to he repaired before the wintr.'\n",
      " 'munors are not allowed to purchase cigarettes or alcohol.'\n",
      " 'the tumor was remove surgicaly.'\n",
      " 'they aquire the company in order to expand their business.'\n",
      " 'the hotel was able to acomodate all of our needs.'\n",
      " 'I marked the dats on my calendar so I would not forger.'\n",
      " 'I asked my collegue for there opinion on the matter.'\n",
      " 'due to the economic situation, main employees were layed off from their jobs.'\n",
      " 'I was refered to the specialist by my primary cate physician.'\n",
      " 'many poets wrote in giambic pentameter.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "555b126e",
   "metadata": {},
   "source": [
    "## Task 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42f26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the nearest neighbors based on edit distance   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b43a1b5-6f5b-43b7-b66e-c9d6ebc54751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [('know', 1), ('knot', 1), ('knob', 1)], 2: [('researchers', 2), ('researcher', 3), ('researches', 3), ('reachers', 3)], 3: [('grown', 1), ('groin', 1), ('groan', 1)], 4: [('quote', 2), ('quota', 2), ('quo', 2), ('quilt', 2), ('quell', 2), ('qualm', 2), ('quila', 2), ('quoad', 2), ('quoit', 2), ('quos', 2)], 5: [('waves', 1), ('wakes', 1), ('waits', 1), ('wages', 1), ('wails', 1), ('wares', 1), ('waxes', 1), ('wades', 1), ('waives', 1), ('waifs', 1), ('wanes', 1)], 6: [('winter', 1), ('wintry', 1), ('with', 2), ('want', 2), ('into', 2), ('went', 2), ('wants', 2), ('win', 2), ('wind', 2), ('wine', 2), ('winner', 2), ('wins', 2), ('wings', 2), ('wing', 2), ('minor', 2), ('hint', 2), ('diner', 2), ('winds', 2), ('ninth', 2), ('wit', 2), ('mint', 2), ('witty', 2), ('wits', 2), ('wiser', 2), ('pint', 2), ('wink', 2), ('finer', 2), ('wider', 2), ('windy', 2), ('intro', 2), ('hints', 2), ('wiener', 2), ('mints', 2), ('wilt', 2), ('wines', 2), ('miner', 2), ('liner', 2), ('pints', 2), ('lint', 2), ('winch', 2), ('wont', 2), ('width', 2), ('Pinto', 2), ('wino', 2), ('winks', 2), ('wiper', 2), ('minty', 2), ('winos', 2), ('inter', 2), ('tint', 2), ('dinar', 2), ('aint', 2), ('whiner', 2), ('wined', 2), ('winery', 2), ('wince', 2), ('Pinta', 2), ('entr', 2), ('winder', 2), ('dint', 2), ('bint', 2), ('int', 2), ('Sinter', 2), ('wanty', 2), ('wilts', 2), ('wite', 2), ('contr', 2), ('linty', 2), ('oint', 2), ('tints', 2), ('Winer', 2), ('winker', 2), ('Wint', 2), ('wintery', 2), ('wist', 2)], 7: [('minors', 1), ('minor', 2), ('rumors', 2), ('honors', 2), ('manor', 2), ('majors', 2), ('miners', 2), ('jurors', 2), ('donors', 2), ('tumors', 2), ('moors', 2), ('tutors', 2), ('juniors', 2), ('mucous', 2), ('monos', 2), ('mayors', 2), ('mentors', 2), ('tenors', 2), ('manos', 2), ('Manors', 2), ('humors', 2), ('lunars', 2), ('Senors', 2), ('Tuners', 2)], 8: [('surgical', 1), ('surgically', 1), ('strictly', 3), ('survival', 3), ('survivals', 3)], 9: [('acquire', 1), ('Squire', 1), ('quire', 1)], 10: [('accomodate', 1), ('accommodate', 2), ('accommodated', 3), ('accommodates', 3), ('abominate', 3)], 11: [('days', 1), ('date', 1), ('dates', 1), ('data', 1), ('cats', 1), ('eats', 1), ('rats', 1), ('hats', 1), ('bats', 1), ('dots', 1), ('dads', 1), ('oats', 1), ('darts', 1), ('mats', 1), ('fats', 1), ('pats', 1), ('dams', 1), ('dais', 1), ('tats', 1), ('Lats', 1), ('dabs', 1), ('gats', 1), ('vats', 1), ('doats', 1), ('Kats', 1), ('dato', 1), ('wats', 1)], 12: [('college', 1), ('colleague', 1), ('colleagues', 2), ('colleges', 2), ('collage', 2)], 13: [('played', 1), ('layer', 1), ('laced', 1), ('Fayed', 1), ('flayed', 1), ('slayed', 1), ('payed', 1), ('lamed', 1), ('lated', 1), ('lazed', 1)], 14: [('care', 1), ('came', 1), ('late', 1), ('case', 1), ('hate', 1), ('date', 1), ('cute', 1), ('cat', 1), ('ate', 1), ('cake', 1), ('rate', 1), ('gate', 1), ('fate', 1), ('mate', 1), ('cage', 1), ('cats', 1), ('cave', 1), ('cafe', 1), ('cane', 1), ('crate', 1), ('cater', 1), ('carte', 1), ('cite', 1), ('Tate', 1), ('pate', 1), ('caste', 1), ('sate', 1), ('bate', 1), ('Cate', 1), ('yate', 1)], 15: [('iambic', 1), ('gambit', 2), ('limbic', 2), ('iambics', 2)]}\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "id=0\n",
    "for misspel in mistakes.values():\n",
    "    neighbors=[]\n",
    "    for word in movie_subtitle:\n",
    "        word = re.findall(r'^[^a-zA-z]*([a-zA-Z]*)[^a-zA-z]*$', str(word))  # \n",
    "        #print(word[0]) <- a word without brackets and ' ' around\n",
    "        if word and word[0] != misspel:\n",
    "            d = nltk.edit_distance(word[0], misspel)\n",
    "            neighbors.append(tuple((word[0],d)))\n",
    "    neighbors.sort(key = lambda x: x[1], reverse=False)\n",
    "    # change it so that it is adding not only 3 but 3 and all the others that have the same edit distance\n",
    "    to_add=neighbors[:3]\n",
    "    third_ele_dist=neighbors[2][1]\n",
    "    for neighbor in neighbors[3:]:\n",
    "        if neighbor[1] ==third_ele_dist:\n",
    "            to_add.append(neighbor)\n",
    "    # if we would put misspel instead of id then we would have the word and not id\n",
    "    output[id+1]=to_add\n",
    "    id+=1\n",
    "        \n",
    "print( output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd317693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to be written -> output\n",
    "with open(\"task3_KatarzynaKleczek_solution.json\", \"w\") as outfile:\n",
    "\tjson.dump(output, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f911707",
   "metadata": {},
   "source": [
    "## Task4: frequency\n",
    "use the list of candidate replacements you found in task3 to find the best one according to candidate frequency (derived from SUBTLEXus) - if two or more candidates have the exact same frequency in SUBTLEX, choose the one with the best edit distance. If two or more candidates at the same frequency also have the same edit distance, pick the one which comes first in alphabetical order. You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its SUBTLEXus frequency value (as an integer). 5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the frequency doesn't match, you loose half a point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b809de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ('know', 291780), 2: ('researchers', 57), 3: ('grown', 1275), 4: ('quote', 488), 5: ('waves', 674), 6: ('with', 257465), 7: ('minor', 654), 8: ('strictly', 548), 9: ('Squire', 157), 10: ('accommodate', 109), 11: ('days', 15592), 12: ('college', 4344), 13: ('played', 2870), 14: ('care', 24748), 15: ('gambit', 19)}\n"
     ]
    }
   ],
   "source": [
    "# pick the best candidate according to SUBTLEXus frequency counts\n",
    "best_replacements={}\n",
    "id=0\n",
    "for element in output.values():\n",
    "    #print( element)\n",
    "    freq_high=0\n",
    "    for i in range(len(element)):\n",
    "        word=element[i][0]\n",
    "        curr_ed=100\n",
    "        #print(word)\n",
    "        curr_substitute=\"\"\n",
    "        row_index=movie_subtitlesraw[movie_subtitlesraw['Word'] == word].index[0]\n",
    "        #print(row_index)\n",
    "        freq=movie_subtitlesraw[\"FREQcount\"][row_index]\n",
    "        if ((freq == freq_high) and (element[i][1] == curr_ed)):\n",
    "            if (word[0]<curr_substitute[0]):\n",
    "                possible_replecements=(word, freq)\n",
    "                freq_high=freq\n",
    "                curr_substitute=word\n",
    "        if (freq > freq_high):\n",
    "            #print(freq)\n",
    "            #print((word, freq))\n",
    "            #possible_replecements=[]\n",
    "            possible_replecements=(word, freq)\n",
    "            freq_high=freq\n",
    "            curr_substitute=word\n",
    "            curr_ed=element[i][1]\n",
    "\n",
    "    best_replacements[id+1]=(possible_replecements)\n",
    "    id+=1\n",
    "\n",
    "print(best_replacements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ba3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cabe2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to be written -> output    needed to add a default np_encoder because otherwise it didn't accept the tupple,\n",
    "# now all tuples are converted into lists but I assume there is no other way around it\n",
    "\n",
    "with open(\"task4_KatarzynaKleczek_solution.json\", \"w\") as outfile:\n",
    "\tjson.dump(best_replacements, outfile, default=np_encoder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1024d5b2",
   "metadata": {},
   "source": [
    "## Task5: perplexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4906ebd",
   "metadata": {},
   "source": [
    "use the list of candidate replacements you found in task3 to find the best one according to its perplexity under a statistical language model of order 3 implemented using a Markov Chain with add-k smoothing (k=0.01) and estimated using the given corpus. If two or more candidates have the exact same perplexity in the input sentence, choose the one with the best edit distance. If two or more candidates at the same perplexity also have the same edit distance, follow alphabetical order. You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its perplexity under the specified language model (as a float). Not all candidate replacements might appear in the LM training corpus: don't map such candidate replacements to the UNK string though, or the perplexity estimate would not reflect the specific candidate; you can directly exclude candidate replacements which don't appear in the training corpus from the perplexity computation. 5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the perplexity doesn't match, you loose half a point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2345b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the best candidate according to perplexity under the given language model\n",
    "# best candidate replacements from task3 -> output.values()  from those using 3gram model with smoothing k=0.01, estimate perplexity\n",
    "new_model=LM(data_trainingCorpus, ngram_size=3, k=0.01, )\n",
    "new_model.update_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c497b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('know', 1002.7288999720108, 1), ('knot', 5639.354069762032, 1), ('knob', 8575.27839748091, 1)], [('researchers', 14475.415924638553, 2), ('researcher', 14476.248088113443, 3), ('researches', 14475.415924638553, 3), ('reachers', 14475.415924638553, 3)], [('grown', 1861.3599787842493, 1), ('groin', 1858.1347148669051, 1), ('groan', 1858.8593594720237, 1)], [('quote', 16820.32265338231, 2), ('quota', 28937.539844403862, 2), ('quo', 28933.1915634287, 2), ('quilt', 19307.938029302248, 2), ('quell', 28933.1915634287, 2), ('qualm', 28946.21295587551, 2), ('quila', 28933.1915634287, 2), ('quoad', 28933.1915634287, 2), ('quoit', 28933.1915634287, 2), ('quos', 28933.1915634287, 2)], [('waves', 1220.1171486647056, 1), ('wakes', 1832.2622055583065, 1), ('waits', 1831.7334704549087, 1), ('wages', 1827.260259071301, 1), ('wails', 1827.3976276540632, 1), ('wares', 1827.260259071301, 1), ('waxes', 1827.5350065638286, 1), ('wades', 1827.260259071301, 1), ('waives', 1827.260259071301, 1), ('waifs', 1827.260259071301, 1), ('wanes', 1827.260259071301, 1)], [('winter', 2147.773768366897, 1), ('wintry', 8219.960238645888, 1), ('with', 5457.273700422706, 2), ('want', 5541.212585560772, 2), ('into', 5958.714744103964, 2), ('went', 5722.395012400671, 2), ('wants', 6009.844237624907, 2), ('win', 4333.3925001443, 2), ('wind', 1857.4043179891894, 2), ('wine', 2409.212747441707, 2), ('winner', 3936.8885081093167, 2), ('wins', 6625.27510054141, 2), ('wings', 3415.748402725805, 2), ('wing', 3888.4758092796073, 2), ('minor', 6980.954364550521, 2), ('hint', 4120.202047767535, 2), ('diner', 3806.262556587309, 2), ('winds', 2950.4109952050853, 2), ('ninth', 5070.030649548264, 2), ('wit', 6515.402135742955, 2), ('mint', 4659.915538373562, 2), ('witty', 6969.735041298211, 2), ('wits', 6612.058895249942, 2), ('wiser', 4017.626054764363, 2), ('pint', 7226.796040151702, 2), ('wink', 4303.658281411412, 2), ('finer', 7650.667976909389, 2), ('wider', 6430.411415075977, 2), ('windy', 7227.261882365566, 2), ('intro', 8217.303731714712, 2), ('hints', 6901.660003497013, 2), ('wiener', 7638.858131216718, 2), ('mints', 5027.70527463821, 2), ('wilt', 7103.6521532220395, 2), ('wines', 7849.330725941123, 2), ('miner', 5267.823418440419, 2), ('liner', 5010.190986954605, 2), ('pints', 8214.099970533987, 2), ('lint', 7225.395985693248, 2), ('winch', 5128.884178501296, 2), ('wont', 11172.570769665605, 2), ('width', 7244.6596567006945, 2), ('Pinto', 11173.298700372055, 2), ('wino', 8215.16983554193, 2), ('winks', 6932.666699270305, 2), ('wiper', 7160.9815870800185, 2), ('minty', 8215.16983554193, 2), ('winos', 11173.298700372055, 2), ('inter', 8215.702472739662, 2), ('tint', 7495.139699098085, 2), ('dinar', 11172.570769665605, 2), ('aint', 11172.570769665605, 2), ('whiner', 7638.360466243728, 2), ('wined', 11172.570769665605, 2), ('winery', 5617.928533994305, 2), ('wince', 6443.882361054692, 2), ('Pinta', 7296.128372802177, 2), ('entr', 11172.570769665605, 2), ('winder', 11172.570769665605, 2), ('dint', 11172.570769665605, 2), ('bint', 11172.570769665605, 2), ('int', 7494.163603916336, 2), ('Sinter', 11172.570769665605, 2), ('wanty', 11172.570769665605, 2), ('wilts', 11172.570769665605, 2), ('wite', 11172.570769665605, 2), ('contr', 11172.570769665605, 2), ('linty', 11172.570769665605, 2), ('oint', 11172.570769665605, 2), ('tints', 11172.570769665605, 2), ('Winer', 11172.570769665605, 2), ('winker', 11172.570769665605, 2), ('Wint', 11172.570769665605, 2), ('wintery', 11172.570769665605, 2), ('wist', 11172.570769665605, 2)], [('minors', 14199.607351154302, 1), ('minor', 14199.607351154302, 2), ('rumors', 9338.085472292762, 2), ('honors', 14199.607351154302, 2), ('manor', 14199.607351154302, 2), ('majors', 14199.607351154302, 2), ('miners', 14202.129404060332, 2), ('jurors', 9336.433652424052, 2), ('donors', 14200.86893749825, 2), ('tumors', 14199.607351154302, 2), ('moors', 14199.607351154302, 2), ('tutors', 14199.607351154302, 2), ('juniors', 14199.607351154302, 2), ('mucous', 14199.607351154302, 2), ('monos', 14199.607351154302, 2), ('mayors', 14199.607351154302, 2), ('mentors', 14199.607351154302, 2), ('tenors', 14199.607351154302, 2), ('manos', 14199.607351154302, 2), ('Manors', 14199.607351154302, 2), ('humors', 14199.607351154302, 2), ('lunars', 14199.607351154302, 2), ('Senors', 14199.607351154302, 2), ('Tuners', 14199.607351154302, 2)], [('surgical', 31362.11127051458, 1), ('surgically', 34597.21194715582, 1), ('strictly', 31362.11127051458, 3), ('survival', 18918.657040618258, 3), ('survivals', 66882.43792919899, 3)], [('acquire', 2192.659500352165, 1), ('Squire', 2190.000743626073, 1), ('quire', 2190.000743626073, 1)], [('accomodate', 2767.2030206738777, 1), ('accommodate', 1897.8469004552046, 2), ('accommodated', 2767.2030206738777, 3), ('accommodates', 2767.2030206738777, 3), ('abominate', 2767.2030206738777, 3)], [('days', 3797.947657551638, 1), ('date', 2274.1923660774823, 1), ('dates', 4995.018376453822, 1), ('data', 5185.0393776805895, 1), ('cats', 3898.330002794193, 1), ('eats', 7488.765892329181, 1), ('rats', 7510.381791460462, 1), ('hats', 5394.330707782133, 1), ('bats', 7503.253126665158, 1), ('dots', 5396.933103757329, 1), ('dads', 7490.854135841328, 1), ('oats', 7490.332784105784, 1), ('darts', 7493.972380181253, 1), ('mats', 5390.9755470007585, 1), ('fats', 7489.810960201678, 1), ('pats', 3689.5422881210898, 1), ('dams', 7489.810960201678, 1), ('dais', 5397.248261482091, 1), ('tats', 7489.288663239995, 1), ('Lats', 7488.765892329181, 1), ('dabs', 7488.765892329181, 1), ('gats', 7488.765892329181, 1), ('vats', 7491.895426350097, 1), ('doats', 7488.765892329181, 1), ('Kats', 7488.765892329181, 1), ('dato', 7488.765892329181, 1), ('wats', 7488.765892329181, 1)], [('college', 1307.6444147113198, 1), ('colleague', 888.999722318408, 1), ('colleagues', 1309.3597557162418, 2), ('colleges', 1303.8538389114717, 2), ('collage', 1303.7476585946565, 2)], [('played', 6631.220181040861, 1), ('layer', 6625.174599990876, 1), ('laced', 6626.791073901103, 1), ('Fayed', 6625.174599990876, 1), ('flayed', 6625.579273379231, 1), ('slayed', 6625.174599990876, 1), ('payed', 6625.174599990876, 1), ('lamed', 6625.174599990876, 1), ('lated', 6625.174599990876, 1), ('lazed', 6625.174599990876, 1)], [('care', 11882.810558938134, 1), ('came', 26272.17497899488, 1), ('late', 26270.200054989145, 1), ('case', 26270.200054989145, 1), ('hate', 26270.200054989145, 1), ('date', 26270.200054989145, 1), ('cute', 26270.200054989145, 1), ('cat', 26270.200054989145, 1), ('ate', 26270.200054989145, 1), ('cake', 26270.200054989145, 1), ('rate', 26270.200054989145, 1), ('gate', 26270.200054989145, 1), ('fate', 26270.200054989145, 1), ('mate', 26270.200054989145, 1), ('cage', 26270.200054989145, 1), ('cats', 26270.200054989145, 1), ('cave', 26270.200054989145, 1), ('cafe', 26270.200054989145, 1), ('cane', 26270.200054989145, 1), ('crate', 26270.200054989145, 1), ('cater', 26270.200054989145, 1), ('carte', 26270.200054989145, 1), ('cite', 26270.200054989145, 1), ('Tate', 26270.200054989145, 1), ('pate', 26270.200054989145, 1), ('caste', 26270.200054989145, 1), ('sate', 26270.200054989145, 1), ('bate', 26270.200054989145, 1), ('Cate', 26270.200054989145, 1), ('yate', 26270.200054989145, 1)], [('iambic', 31291.168009478923, 1), ('gambit', 123769.33625520598, 2), ('limbic', 123769.33625520598, 2), ('iambics', 123769.33625520598, 2)]]\n"
     ]
    }
   ],
   "source": [
    "perpl_list=[]\n",
    "id=0\n",
    "for sth in output.values():\n",
    "    sentence = sentences_tokenized[id]\n",
    "    #print(sentence)\n",
    "    mistake_index=where_are_mistakes1[id]\n",
    "    #print(sth)\n",
    "    replacements=[]\n",
    "    for item in sth:\n",
    "        candidate=item[0]\n",
    "        sentence[mistake_index] = candidate\n",
    "        perp=new_model.perplexity(sentence)\n",
    "        ed=item[1]\n",
    "        # a list with a replacement, its preplexity and the smallest edit distance\n",
    "        replacements.append((candidate, perp, ed))\n",
    "    perpl_list.append(replacements)\n",
    "    id+=1\n",
    "          \n",
    "print(perpl_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45c20231",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictionary={}\n",
    "id=0\n",
    "for element in perpl_list:\n",
    "    element.sort(key = lambda x: x[1], reverse=False)\n",
    "    #print(element)\n",
    "    curr_prep=element[0][1]\n",
    "    #checking if the first element has the lowest preplexity\n",
    "    if  curr_prep< element[1][1]:\n",
    "        # if yes then add it to the final dictionary, the word and the preplexity\n",
    "        final_dictionary[id+1]=(element[0][0], element[0][1])\n",
    "    else:\n",
    "        #create a list of all the words with the same preplexity\n",
    "        temp_list=[]\n",
    "        for prep in element:\n",
    "            if prep[1] == curr_prep:\n",
    "                temp_list.append(prep)\n",
    "        #now we want to sort them by the edit distance\n",
    "        temp_list.sort(key = lambda x: x[2], reverse=False)\n",
    "        # now checking if the first two have the same edit distance\n",
    "        curr_ed=temp_list[0][2]\n",
    "        if curr_ed<temp_list[1][2]:\n",
    "            # if yes, this is our best replacement\n",
    "            final_dictionary[id+1]=(temp_list[0][0],temp_list[0][1])\n",
    "        else:\n",
    "            #create a list of all the words with the same edit distance\n",
    "            temp_list2=[]\n",
    "            for dist in temp_list:\n",
    "                if dist[2] == curr_ed:\n",
    "                    temp_list2.append(dist)\n",
    "            #now we want to sort them alphabetically\n",
    "            temp_list2.sort(key = lambda x: x[0], reverse=False)\n",
    "\n",
    "            # first element of that list is our best distance and should be added to dictionary\n",
    "            final_dictionary[id+1]=(temp_list2[0][0], temp_list2[0][1])\n",
    "    id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96813e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ('know', 1002.7288999720108))\n",
      "(2, ('researchers', 14475.415924638553))\n",
      "(3, ('groin', 1858.1347148669051))\n",
      "(4, ('quote', 16820.32265338231))\n",
      "(5, ('waves', 1220.1171486647056))\n",
      "(6, ('wind', 1857.4043179891894))\n",
      "(7, ('jurors', 9336.433652424052))\n",
      "(8, ('survival', 18918.657040618258))\n",
      "(9, ('Squire', 2190.000743626073))\n",
      "(10, ('accommodate', 1897.8469004552046))\n",
      "(11, ('date', 2274.1923660774823))\n",
      "(12, ('colleague', 888.999722318408))\n",
      "(13, ('Fayed', 6625.174599990876))\n",
      "(14, ('care', 11882.810558938134))\n",
      "(15, ('iambic', 31291.168009478923))\n",
      "{1: ('know', 1002.7288999720108), 2: ('researchers', 14475.415924638553), 3: ('groin', 1858.1347148669051), 4: ('quote', 16820.32265338231), 5: ('waves', 1220.1171486647056), 6: ('wind', 1857.4043179891894), 7: ('jurors', 9336.433652424052), 8: ('survival', 18918.657040618258), 9: ('Squire', 2190.000743626073), 10: ('accommodate', 1897.8469004552046), 11: ('date', 2274.1923660774823), 12: ('colleague', 888.999722318408), 13: ('Fayed', 6625.174599990876), 14: ('care', 11882.810558938134), 15: ('iambic', 31291.168009478923)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for element in final_dictionary.items():\n",
    "    print(element)\n",
    "\n",
    "print(final_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41e15a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task5_KatarzynaKleczek_solution.json\", \"w\") as outfile:\n",
    "\tjson.dump(final_dictionary, outfile, default=np_encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8513369b",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "(add your comments here)\n",
    "Compare the candidate replacements found when considering frequency and when considering perplexity. Which are better? Do they match what you consider to be the right replacement? What extra resources/information would you use to pick better candidate replacements? 3 points available, awarded based on how sensible the answer is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73260eed",
   "metadata": {},
   "source": [
    "It looks like based on the frequency the replacements are more accurate (but it is just 3:2). However it is not always the case, for example in the sentence \"the roofs of the old house needed to he repaired before the wintr.\" the replacement should be in my opinion \"winter\" but that is not the replacemetn choosen by any of the algorithms. Sometimes neither is correct. I think depending on the corpus and also smoothing or n-gram size used, perplexity can do much better than frequency. Some fine tuning of the parameters should be used to find out if that is indeed correct for this corpus. Another fact about those cases is that when some words had the same perplexity, we chose alphabetic orrder. That is a criteria that we could use but can change the output completely. Other option would be to do it randomly for exmaple but we habe no guarantee that it would be any better than the alphabetic order and now at least we can backtrack it and be sure why this exact word was choosen to be a replacement. In terms of extra information that could be used is for example the Parts of Speach parts, although here it does not seem to have a huge part in terms of making a difference between frequency and perplexity, it is nevertheless something that can add soem context and infromation to our model and help with choosing a correct word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dc92eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I could not help but grozn in frustration when my computer trashed right before I finished my project.\n",
      "grozn ('groin', 1858.1347148669051) ('grown', 1275)\n",
      "\n",
      "the roofs of the old house needed to he repaired before the wintr.\n",
      "wintr ('wind', 1857.4043179891894) ('with', 257465)\n",
      "\n",
      "munors are not allowed to purchase cigarettes or alcohol.\n",
      "munors ('jurors', 9336.433652424052) ('minor', 654)\n",
      "\n",
      "the tumor was remove surgicaly.\n",
      "surgicaly ('survival', 18918.657040618258) ('strictly', 548)\n",
      "\n",
      "I marked the dats on my calendar so I would not forger.\n",
      "dats ('date', 2274.1923660774823) ('days', 15592)\n",
      "\n",
      "I asked my collegue for there opinion on the matter.\n",
      "collegue ('colleague', 888.999722318408) ('college', 4344)\n",
      "\n",
      "due to the economic situation, main employees were layed off from their jobs.\n",
      "layed ('Fayed', 6625.174599990876) ('played', 2870)\n",
      "\n",
      "many poets wrote in giambic pentameter.\n",
      "giambic ('iambic', 31291.168009478923) ('gambit', 19)\n"
     ]
    }
   ],
   "source": [
    "prep=final_dictionary\n",
    "freq=best_replacements\n",
    "\n",
    "for i in range(len(prep.items())):\n",
    "    if(prep[i][0]==freq[i][0]):\n",
    "        #print(mistakes[i], \": the same replacement found\")\n",
    "        continue\n",
    "    else:\n",
    "        print()\n",
    "        print(sentences[i])\n",
    "        print(mistakes[i], prep[i], freq[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5070db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
